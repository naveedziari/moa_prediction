{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utpzby6Ax0Dg",
        "outputId": "f4b4c63f-70a9-472a-c3ec-1db5331471e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting iterative-stratification\n",
            "  Downloading iterative_stratification-0.1.7-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.21.6)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (3.1.0)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.7\n"
          ]
        }
      ],
      "source": [
        "!pip install iterative-stratification\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDQtW1z5yvWS"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('train_features.csv')\n",
        "target_df = pd.read_csv('train_targets_scored.csv')\n",
        "test_df = pd.read_csv('test_features.csv')\n",
        "drug_df = pd.read_csv('train_drug.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqNPanxsrRIu"
      },
      "outputs": [],
      "source": [
        "#make sure everything is in order\n",
        "assert (list(train_df['sig_id']) == list(target_df['sig_id']))\n",
        "sig_id = list(train_df['sig_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRr8bSkRzFZS"
      },
      "outputs": [],
      "source": [
        "#create dummy variables for categorical features and drop sig_id\n",
        "def dumb(df):\n",
        "    df['cp_type'] = df['cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
        "    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n",
        "    df = df.drop(['sig_id'],axis = 1)\n",
        "    return df\n",
        "\n",
        "\n",
        "train_df = dumb(train_df)\n",
        "test_df = dumb(test_df)\n",
        "#target_df = target_df.drop(['sig_id'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zqB-SY31bpg"
      },
      "outputs": [],
      "source": [
        "#Remove control subjects. They have zero MoA and are given drug cacb2b860 (DMSO)\n",
        "target_df = target_df.loc[train_df['cp_type'] == 0].reset_index(drop=True) #also reflect in the target matrix\n",
        "drug_df = drug_df.loc[train_df['cp_type'] == 0].reset_index(drop=True)\n",
        "train_df = train_df.loc[train_df['cp_type'] == 0].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNotWSVt21GD"
      },
      "outputs": [],
      "source": [
        "#add PCA as features to train and testing set\n",
        "# pca = PCA(n_components = 256)\n",
        "# pca_array = pca.fit_transform(train_df.to_numpy())\n",
        "# train_df = pd.concat([train_df,pd.DataFrame(pca_array,columns = ['PC' + str(i) for i in range(pca_array.shape[1])])],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afUlVvQT2Okv"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataloader import T\n",
        "#selection of top features\n",
        "vt_fct = VarianceThreshold(threshold=0.8)\n",
        "vt_fct.fit(train_df.to_numpy()[:,3:]) #exclude categorical features\n",
        "\n",
        "temp = vt_fct.get_support(indices = True)\n",
        "temp = temp + 3\n",
        "temp = np.copy(np.concatenate([np.array([1,2]), temp]))\n",
        "top_features = temp.tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcRCxrfI1nOC"
      },
      "source": [
        "**HYPERPARAMETERS:**\n",
        "- number of epochs\n",
        "- dropout of each layer\n",
        "- number of nodes\n",
        "- number of hidden layers\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIy-vdlwLXDh"
      },
      "outputs": [],
      "source": [
        "def create_folds(num_start,num_splits, threshold):\n",
        "    '''\n",
        "    implementation of k-fold cross validation and stratification of drugs without much targets\n",
        "    next step - remove drugs that have little targets?\n",
        "\n",
        "    theshold for target = 16? how to best choose threshold?\n",
        "    '''\n",
        "    folds = []\n",
        "\n",
        "    # LOAD FILES\n",
        "    train_df = pd.read_csv('train_features.csv')\n",
        "    score_df = pd.read_csv('train_targets_scored.csv')\n",
        "    drug_df = pd.read_csv('train_drug.csv')\n",
        "    #remove controls\n",
        "    score_df = score_df.loc[train_df['cp_type'] == 'trt_cp',:]\n",
        "    drug_df = drug_df.loc[train_df['cp_type'] == 'trt_cp',:]\n",
        "    targets = score_df.columns[1:] #remove sig_id\n",
        "    score_df = score_df.merge(drug_df,on = 'sig_id', how = 'left')\n",
        "\n",
        "    drug_counts = score_df['drug_id'].value_counts()\n",
        "\n",
        "    below_threshold = drug_counts.loc[drug_counts <= threshold].index.sort_values()\n",
        "    above_threshold = drug_counts.loc[drug_counts > threshold].index.sort_values()\n",
        "\n",
        "    # train_feats = pd.read_csv('train_features.csv')\n",
        "    # scored = pd.read_csv('train_targets_scored.csv')\n",
        "    # drug = pd.read_csv('train_drug.csv')\n",
        "    # scored = scored.loc[train_feats['cp_type'] == 'trt_cp', :]\n",
        "    # drug = drug.loc[train_feats['cp_type'] == 'trt_cp', :]\n",
        "    # targets = scored.columns[1:]\n",
        "    # scored = scored.merge(drug, on='sig_id', how='left') \n",
        "\n",
        "    #find drug counts for each moa\n",
        "    #score_df = target_df.merge(drug_df, on = 'sig_id', how = 'left')\n",
        "        \n",
        "\n",
        "    for seed in range(num_start):\n",
        "\n",
        "        dict_b = {} #below_threshold\n",
        "        dict_a = {} #above threshold\n",
        "        #stratification of drugs below threshold\n",
        "        xval_model = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n",
        "        temp = score_df.groupby('drug_id')[targets].mean().loc[below_threshold]\n",
        "\n",
        "        for fold,(T,V) in enumerate(xval_model.split(temp,temp[targets])):\n",
        "            dict_append = {k:fold for k in temp.index[V].values}\n",
        "            dict_b.update(dict_append)\n",
        "\n",
        "        #stratification of drugs above threshold\n",
        "        xval_model = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n",
        "        temp = score_df.loc[score_df['drug_id'].isin(above_threshold)].reset_index(drop = True)\n",
        "        for fold,(T,V) in enumerate(xval_model.split(temp,temp[targets])):\n",
        "            dict_append = {k:fold for k in temp['sig_id'][V].values}\n",
        "            dict_a.update(dict_append)\n",
        "\n",
        "        #assign folds\n",
        "        score_df['fold'] = score_df['drug_id'].map(dict_b)\n",
        "        score_df.loc[score_df['fold'].isna(),'fold'] = score_df.loc[score_df['fold'].isna(),'sig_id'].map(dict_a)\n",
        "        score_df['fold'] = score_df['fold'].astype('int8')\n",
        "        folds.append(score_df['fold'].values)\n",
        "\n",
        "        del score_df['fold']\n",
        "\n",
        "    return np.stack(folds)\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQvCted_a9I-"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsVoX1Uje2hW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epj-uvSyifPn"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=16)\n",
        "pca_array = pca.fit_transform(train_df.to_numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXYyMkuYi2DY"
      },
      "outputs": [],
      "source": [
        "# for i,col_name in enumerate(['PC' + str(i+1) for i in range(16)]):\n",
        "#     train_df[col_name] = pca_array[:,i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8T3oO4jbEMz"
      },
      "outputs": [],
      "source": [
        "n_fold = 4\n",
        "n_start = 4\n",
        "n_epoch = 16\n",
        "threshold = 16\n",
        "random_state = 42 #the meaning of life\n",
        "\n",
        "batch_size = 512\n",
        "val_batch_size = batch_size * 4\n",
        "\n",
        "n_targets = target_df.shape[1] -1 \n",
        "targets = [col for col in target_df.columns][1:]\n",
        "\n",
        "criterion = nn.BCELoss()  # Binary Cross Entropy loss function\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "folds_cv = create_folds(n_start, n_fold, threshold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fc-31NWFySL"
      },
      "outputs": [],
      "source": [
        "folds_cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuwQa4VVCSr3"
      },
      "outputs": [],
      "source": [
        "unique, counts = np.unique(folds_cv[0], return_counts=True)\n",
        "for i in unique:\n",
        "    print(i, counts[i])\n",
        "del unique\n",
        "del counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK8-HYChhzDn"
      },
      "outputs": [],
      "source": [
        "class MoaModel(nn.Module):\n",
        "    def __init__(self, num_columns):\n",
        "        super(MoaModel, self).__init__()\n",
        "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
        "        self.dropout1 = nn.Dropout(0.2) #train hyperparameter \n",
        "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 1024))\n",
        "        \n",
        "        self.batch_norm2 = nn.BatchNorm1d(1024)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.dense2 = nn.utils.weight_norm(nn.Linear(1024, 1024))\n",
        "\n",
        "        self.batch_norm3 = nn.BatchNorm1d(1024)\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "        self.dense3 = nn.utils.weight_norm(nn.Linear(1024, 206))\n",
        "        \n",
        "        # self.batch_norm4 = nn.BatchNorm1d(1024)\n",
        "        # self.dropout4 = nn.Dropout(0.2)\n",
        "        # self.dense4 = nn.utils.weight_norm(nn.Linear(1024, 206))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.dense1(x))\n",
        "        \n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.dense2(x))\n",
        "\n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = F.sigmoid(self.dense3(x))\n",
        "        \n",
        "        # x = self.batch_norm4(x)\n",
        "        # x = self.dropout4(x)\n",
        "        # x = F.sigmoid(self.dense4(x))\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLGc-bq0bQBz"
      },
      "outputs": [],
      "source": [
        "#create space for hyperparameter, input is model class, amazon sagemaker for choice of hyperparameter\n",
        "#grid search, random search, bayesian search, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d92Rv01XiNR2"
      },
      "outputs": [],
      "source": [
        "class MoaDataset(Dataset):\n",
        "    def __init__(self, df, targets, feats_idx, mode='train'):\n",
        "        self.mode = mode\n",
        "        self.feats = feats_idx\n",
        "        self.data = df[:, feats_idx]\n",
        "        if mode=='train':\n",
        "            self.targets = targets\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == 'train':\n",
        "            return torch.FloatTensor(self.data[idx]), torch.FloatTensor(self.targets[idx])\n",
        "        elif self.mode == 'test':\n",
        "            return torch.FloatTensor(self.data[idx]), 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KACI0Pbie6F_"
      },
      "outputs": [],
      "source": [
        "loss_array = np.zeros((n_start,n_epoch))\n",
        "val_array = np.zeros((n_start,n_epoch))\n",
        "\n",
        "counter = 0\n",
        "for seed in range(n_start):\n",
        "    \n",
        "    print(f'Train seed {seed}')\n",
        "    set_seed(seed)\n",
        "    for fold in range(n_fold):\n",
        "        tr_idx = folds_cv[seed] != fold\n",
        "        te_idx = folds_cv[seed] == fold\n",
        "\n",
        "        xtrain = train_df.to_numpy()[tr_idx]\n",
        "        ytrain = target_df.drop(['sig_id'],axis=1).to_numpy()[tr_idx]\n",
        "        xval = train_df.to_numpy()[te_idx]\n",
        "        yval = target_df.drop(['sig_id'],axis=1).to_numpy()[te_idx]\n",
        "\n",
        "        #print(ytrain)\n",
        "\n",
        "        train_set = MoaDataset(xtrain, ytrain, top_features)\n",
        "        val_set = MoaDataset(xval, yval, top_features)\n",
        "        \n",
        "        dataloaders = {\n",
        "            'train': DataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
        "            'val': DataLoader(val_set, batch_size=val_batch_size, shuffle=False)\n",
        "        }\n",
        "\n",
        "        model = MoaModel(len(top_features)).to(device)\n",
        "        checkpoint_path = f'repeat:{seed}_Fold:{fold+1}.pt'\n",
        "\n",
        "\n",
        "        optimizer = optim.AdamW(model.parameters(), weight_decay=1e-4)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
        "                                                         factor=0.1, patience=3, \n",
        "                                                         eps=1e-4, verbose=True)\n",
        "        best_loss = {'train': np.inf, 'val': np.inf}\n",
        "        \n",
        "        for epoch in range(n_epoch):\n",
        "            epoch_loss = {'train': 0.0, 'val': 0.0}\n",
        "          \n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()\n",
        "                else:\n",
        "                    model.eval()\n",
        "               \n",
        "                running_loss = 0.0\n",
        "                \n",
        "                for i, (x, y) in enumerate(dataloaders[phase]):\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    \n",
        "                    optimizer.zero_grad()\n",
        "                    \n",
        "                    with torch.set_grad_enabled(phase=='train'):\n",
        "                        preds = model(x)\n",
        "                        loss = criterion(preds, y)\n",
        "\n",
        "                        \n",
        "                        if phase=='train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "                        \n",
        "                    running_loss += loss.item() / len(dataloaders[phase])\n",
        "                    \n",
        "\n",
        "\n",
        "                epoch_loss[phase] = running_loss\n",
        "\n",
        "        \n",
        "            print(\"Epoch {}/{}   -   loss: {:5.5f}   -   val_loss: {:5.5f}\".format(epoch+1, n_epoch, epoch_loss['train'], epoch_loss['val']))\n",
        "\n",
        "            loss_array[counter,epoch] = epoch_loss['train']\n",
        "            val_array[counter,epoch] = epoch_loss['val']\n",
        "            \n",
        "            scheduler.step(epoch_loss['val'])\n",
        "            \n",
        "            if epoch_loss['val'] < best_loss['val']:\n",
        "                best_loss = epoch_loss\n",
        "                torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "\n",
        "    \n",
        "    counter += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jH9XiPmomUE"
      },
      "outputs": [],
      "source": [
        "#initiate matrices holding data\n",
        "test_np = test_df.to_numpy()\n",
        "\n",
        "oof = np.zeros((len(train_df.to_numpy()), n_start, n_targets))\n",
        "oof_targets = np.zeros((len(train_df.to_numpy()), n_targets))\n",
        "preds = np.zeros((len(test_np), n_targets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaDd_Gepu9Zc"
      },
      "outputs": [],
      "source": [
        "def mean_log_loss(y_true, y_pred):\n",
        "    metrics = []\n",
        "    worst_target = None\n",
        "    worst_loss = 0.\n",
        "    all_targets_ll = {}\n",
        "    for i, target in enumerate(targets):\n",
        "        _ll = log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1])\n",
        "        metrics.append(_ll)\n",
        "        all_targets_ll[target] = _ll\n",
        "        if _ll > worst_loss:\n",
        "            worst_loss = _ll\n",
        "            worst_target = target\n",
        "    return np.mean(metrics), (worst_target, worst_loss), all_targets_ll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGPxp_SmvDJI"
      },
      "outputs": [],
      "source": [
        "worst_targets_in_seed = []\n",
        "all_targets_ll_in_seed = []\n",
        "for seed in range(n_start):\n",
        "    print(f\"Inference for seed {seed}\")\n",
        "    seed_targets = []\n",
        "    seed_oof = []\n",
        "    seed_preds = np.zeros((len(test_np), n_targets, n_fold))\n",
        "    print(seed_preds.shape)\n",
        "    \n",
        "    for fold in range(n_fold):\n",
        "        # print(n, len(tr))\n",
        "        tr_idx = folds_cv[seed] != fold\n",
        "        te_idx = folds_cv[seed] == fold\n",
        "        xval = train_df.to_numpy()[te_idx]\n",
        "        yval = target_df.drop(['sig_id'],axis=1).to_numpy()[te_idx]\n",
        "        fold_preds = []\n",
        "        \n",
        "        val_set = MoaDataset(xval, yval, top_features)\n",
        "        test_set = MoaDataset(test_np, None, top_features, mode='test')\n",
        "        \n",
        "        dataloaders = {\n",
        "            'val': DataLoader(val_set, batch_size=val_batch_size, shuffle=False),\n",
        "            'test': DataLoader(test_set, batch_size=val_batch_size, shuffle=False)\n",
        "        }\n",
        "        \n",
        "        checkpoint_path = f'repeat:{seed}_Fold:{fold+1}.pt'\n",
        "        model = MoaModel(len(top_features)).to(device)\n",
        "        model.load_state_dict(torch.load(checkpoint_path))\n",
        "        model.eval()\n",
        "        \n",
        "        for phase in ['val', 'test']:\n",
        "            for i, (x, y) in enumerate(dataloaders[phase]):\n",
        "                # print(i)\n",
        "                if phase == 'val':\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                elif phase == 'test':\n",
        "                    x = x.to(device)\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    batch_preds = model(x)\n",
        "                    \n",
        "                    if phase == 'val':\n",
        "                        seed_targets.append(y)\n",
        "                        seed_oof.append(batch_preds)\n",
        "                        # print(y_pred_lr.shape)\n",
        "                    elif phase == 'test':\n",
        "                        fold_preds.append(batch_preds)\n",
        "                    \n",
        "        fold_preds = torch.cat(fold_preds, dim=0).cpu().numpy()\n",
        "        seed_preds[:, :, fold] = fold_preds\n",
        "        \n",
        "    seed_targets = torch.cat(seed_targets, dim=0).cpu().numpy()\n",
        "    seed_oof = torch.cat(seed_oof, dim=0).cpu().numpy()\n",
        "    seed_preds = np.mean(seed_preds, axis=2)\n",
        "    \n",
        "    #print(\"Score for this seed {:5.5f}\".format(mean_log_loss(seed_targets, seed_oof)[0]))\n",
        "    worst_targets_in_seed.append(mean_log_loss(seed_targets, seed_oof)[1])\n",
        "    all_targets_ll_in_seed.append(mean_log_loss(seed_targets, seed_oof)[2])\n",
        "    oof_targets = seed_targets\n",
        "    oof[:, seed, :] = seed_oof\n",
        "    preds += seed_preds / n_start\n",
        "\n",
        "oof = np.mean(oof, axis=1)\n",
        "print(\"Overall score is {:5.5f}\".format(mean_log_loss(oof_targets, oof)[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adNsicB2PUKz"
      },
      "outputs": [],
      "source": [
        "ss = pd.read_csv('sample_submission.csv')\n",
        "ss[targets] = preds\n",
        "ss.loc[test_df['cp_type']== 1, targets] = 0\n",
        "ss.to_csv('submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "nn_nz.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}