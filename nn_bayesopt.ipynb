{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn_bayesopt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3spWT4Vvb9Sq",
        "outputId": "2542402c-54f8-4f7b-df7d-0bdbac0f0719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: iterative-stratification in /usr/local/lib/python3.7/dist-packages (0.1.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from iterative-stratification) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->iterative-stratification) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install iterative-stratification\n",
        "!pip install bayesian-optimization\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('train_features.csv')\n",
        "target_df = pd.read_csv('train_targets_scored.csv')\n",
        "test_df = pd.read_csv('test_features.csv')\n",
        "drug_df = pd.read_csv('train_drug.csv')\n"
      ],
      "metadata": {
        "id": "RKlJaaDhcBpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dumb(df):\n",
        "    df['cp_type'] = df['cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
        "    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n",
        "    df = df.drop(['sig_id'],axis = 1)\n",
        "    return df\n",
        "\n",
        "\n",
        "train_df = dumb(train_df)\n",
        "test_df = dumb(test_df)\n",
        "target_df = target_df.drop(['sig_id'],axis=1)\n",
        "n_targets = target_df.shape[1]\n",
        "targets = [col for col in target_df.columns]"
      ],
      "metadata": {
        "id": "7oaTzPl5cBuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove control subjects. They have zero MoA and are given drug cacb2b860 (DMSO)\n",
        "target_df = target_df.loc[train_df['cp_type'] == 0].reset_index(drop=True) #also reflect in the target matrix\n",
        "drug_df = drug_df.loc[train_df['cp_type'] == 0].reset_index(drop=True)\n",
        "train_df = train_df.loc[train_df['cp_type'] == 0].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "1l3GLEpWcU4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add PCA features"
      ],
      "metadata": {
        "id": "Fl7-Ux9CcZbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataloader import T\n",
        "#selection of top features\n",
        "vt_fct = VarianceThreshold(threshold=0.8)\n",
        "vt_fct.fit(train_df.to_numpy()[:,3:]) #exclude categorical features\n",
        "\n",
        "temp = vt_fct.get_support(indices = True)\n",
        "temp = temp + 3\n",
        "temp = np.copy(np.concatenate([np.array([1,2]), temp]))\n",
        "top_features = temp.tolist()"
      ],
      "metadata": {
        "id": "AH5xyWBWcbjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_folds(num_splits, threshold):\n",
        "    '''\n",
        "    implementation of k-fold cross validation and stratification of drugs without much targets\n",
        "    next step - remove drugs that have little targets?\n",
        "\n",
        "    theshold for target = 16? how to best choose threshold?\n",
        "    '''\n",
        "    folds = []\n",
        "\n",
        "    # LOAD FILES\n",
        "    train_df = pd.read_csv('train_features.csv')\n",
        "    score_df = pd.read_csv('train_targets_scored.csv')\n",
        "    drug_df = pd.read_csv('train_drug.csv')\n",
        "    #remove controls\n",
        "    score_df = score_df.loc[train_df['cp_type'] == 'trt_cp',:]\n",
        "    drug_df = drug_df.loc[train_df['cp_type'] == 'trt_cp',:]\n",
        "    targets = score_df.columns[1:] #remove sig_id\n",
        "    score_df = score_df.merge(drug_df,on = 'sig_id', how = 'left')\n",
        "\n",
        "    drug_counts = score_df['drug_id'].value_counts()\n",
        "\n",
        "    below_threshold = drug_counts.loc[drug_counts <= threshold].index.sort_values()\n",
        "    above_threshold = drug_counts.loc[drug_counts > threshold].index.sort_values()\n",
        "\n",
        "\n",
        "    dict_b = {} #below_threshold\n",
        "    dict_a = {} #above threshold\n",
        "    #stratification of drugs below threshold\n",
        "    xval_model = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = 0)\n",
        "    temp = score_df.groupby('drug_id')[targets].mean().loc[below_threshold]\n",
        "\n",
        "    for fold,(T,V) in enumerate(xval_model.split(temp,temp[targets])):\n",
        "        dict_append = {k:fold for k in temp.index[V].values}\n",
        "        dict_b.update(dict_append)\n",
        "\n",
        "    #stratification of drugs above threshold\n",
        "    xval_model = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = 0)\n",
        "    temp = score_df.loc[score_df['drug_id'].isin(above_threshold)].reset_index(drop = True)\n",
        "    for fold,(T,V) in enumerate(xval_model.split(temp,temp[targets])):\n",
        "        dict_append = {k:fold for k in temp['sig_id'][V].values}\n",
        "        dict_a.update(dict_append)\n",
        "\n",
        "    #assign folds\n",
        "    score_df['fold'] = score_df['drug_id'].map(dict_b)\n",
        "    score_df.loc[score_df['fold'].isna(),'fold'] = score_df.loc[score_df['fold'].isna(),'sig_id'].map(dict_a)\n",
        "    score_df['fold'] = score_df['fold'].astype('int8')\n",
        "    folds.append(score_df['fold'].values)\n",
        "\n",
        "    return folds[0]"
      ],
      "metadata": {
        "id": "oAfA1FHnci4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)"
      ],
      "metadata": {
        "id": "y73SDAmHc4ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "i9Lt39j9c5Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MoaModel(nn.Module):\n",
        "    def __init__(self, num_columns):\n",
        "        super(MoaModel, self).__init__()\n",
        "        self.batch_norm1 = nn.BatchNorm1d(num_columns)\n",
        "        self.dropout1 = nn.Dropout(0.2) #train hyperparameter \n",
        "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_columns, 1024))\n",
        "        \n",
        "        self.batch_norm2 = nn.BatchNorm1d(1024)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.dense2 = nn.utils.weight_norm(nn.Linear(1024, 1024))\n",
        "\n",
        "        self.batch_norm3 = nn.BatchNorm1d(1024)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        self.dense3 = nn.utils.weight_norm(nn.Linear(1024, 206))\n",
        "        \n",
        "        # self.batch_norm4 = nn.BatchNorm1d(1024)\n",
        "        # self.dropout4 = nn.Dropout(0.4)\n",
        "        # self.dense4 = nn.utils.weight_norm(nn.Linear(1024, 206))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.dense1(x))\n",
        "        \n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.dense2(x))\n",
        "\n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = F.sigmoid(self.dense3(x))\n",
        "        \n",
        "        # x = self.batch_norm4(x)\n",
        "        # x = self.dropout4(x)\n",
        "        # x = F.sigmoid(self.dense4(x))\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "afFoaWW5c7xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MoaDataset(Dataset):\n",
        "    def __init__(self, df, targets, feats_idx, mode='train'):\n",
        "        self.mode = mode\n",
        "        self.feats = feats_idx\n",
        "        self.data = df[:, feats_idx]\n",
        "        if mode=='train':\n",
        "            self.targets = targets\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode == 'train':\n",
        "            return torch.FloatTensor(self.data[idx]), torch.FloatTensor(self.targets[idx])\n",
        "        elif self.mode == 'test':\n",
        "            return torch.FloatTensor(self.data[idx]), 0"
      ],
      "metadata": {
        "id": "-MJwhsbLdI8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_log_loss(y_true, y_pred):\n",
        "    metrics = []\n",
        "    worst_target = None\n",
        "    worst_loss = 0.\n",
        "    all_targets_ll = {}\n",
        "    for i, target in enumerate(targets):\n",
        "        _ll = log_loss(y_true[:, i], y_pred[:, i].astype(float), labels=[0,1])\n",
        "        metrics.append(_ll)\n",
        "        all_targets_ll[target] = _ll\n",
        "        if _ll > worst_loss:\n",
        "            worst_loss = _ll\n",
        "            worst_target = target\n",
        "    return np.mean(metrics), (worst_target, worst_loss), all_targets_ll"
      ],
      "metadata": {
        "id": "j6PcRRX5ljCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(n_epoch,tr,learning_rate,batch_size):\n",
        "\n",
        "   \n",
        "    #counter = 0\n",
        "    criterion = nn.BCELoss()\n",
        "    seed = 0\n",
        "    set_seed(0)\n",
        "    n_epoch = int(np.round(n_epoch))\n",
        "    batch_size = int(np.round(batch_size))\n",
        "    val_batch_size = batch_size * 4\n",
        "    tr = int(np.round(tr))\n",
        "    n_fold = 4\n",
        "    folds_cv = create_folds(n_fold,tr)\n",
        "\n",
        "    for fold in range(4): \n",
        "\n",
        "        tr_idx = folds_cv != fold\n",
        "        te_idx = folds_cv == fold\n",
        "\n",
        "        xtrain = train_df.to_numpy()[tr_idx]\n",
        "        ytrain = target_df.to_numpy()[tr_idx]\n",
        "        xval = train_df.to_numpy()[te_idx]\n",
        "        yval = target_df.to_numpy()[te_idx]\n",
        "\n",
        "\n",
        "        train_set = MoaDataset(xtrain, ytrain, top_features)\n",
        "        val_set = MoaDataset(xval, yval, top_features)\n",
        "            \n",
        "        dataloaders = {\n",
        "            'train': DataLoader(train_set, batch_size=batch_size, shuffle=True),\n",
        "            'val': DataLoader(val_set, batch_size=val_batch_size, shuffle=False)\n",
        "        }\n",
        "\n",
        "        model = MoaModel(len(top_features)).to(device)\n",
        "        checkpoint_path = f'Fold:{fold+1}.pt'\n",
        "\n",
        "\n",
        "        optimizer = optim.AdamW(model.parameters(), weight_decay=1e-4, lr = learning_rate)\n",
        "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
        "                                                            factor=0.1, patience=3, \n",
        "                                                            eps=1e-4, verbose=True)\n",
        "        best_loss = {'train': np.inf, 'val': np.inf}\n",
        "            \n",
        "        for epoch in range(n_epoch):\n",
        "            epoch_loss = {'train': 0.0, 'val': 0.0}\n",
        "            \n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()\n",
        "                else:\n",
        "                    model.eval()\n",
        "                \n",
        "                running_loss = 0.0\n",
        "                \n",
        "                for i, (x, y) in enumerate(dataloaders[phase]):\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    \n",
        "                    optimizer.zero_grad()\n",
        "                    \n",
        "                    with torch.set_grad_enabled(phase=='train'):\n",
        "                        preds = model(x)\n",
        "                        loss = criterion(preds, y)\n",
        "                        \n",
        "                        if phase=='train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "                        \n",
        "                    running_loss += loss.item() / len(dataloaders[phase])\n",
        "                \n",
        "                epoch_loss[phase] = running_loss\n",
        "\n",
        "                    \n",
        "\n",
        "            print(\"Epoch {}/{}   -   loss: {:5.5f}   -   val_loss: {:5.5f}\".format(epoch+1, n_epoch, epoch_loss['train'], epoch_loss['val']))\n",
        "\n",
        "            # loss_array[counter,epoch] = epoch_loss['train']\n",
        "            # val_array[counter,epoch] = epoch_loss['val']\n",
        "            \n",
        "            scheduler.step(epoch_loss['val'])\n",
        "            \n",
        "            if epoch_loss['val'] < best_loss['val']:\n",
        "                best_loss = epoch_loss\n",
        "                torch.save(model.state_dict(), checkpoint_path)\n",
        "        #counter += 1\n",
        "\n",
        "\n",
        "    test_np = test_df.to_numpy()\n",
        "\n",
        "    oof = np.zeros((len(test_df.to_numpy()), n_targets))\n",
        "    oof_targets = np.zeros((len(test_df.to_numpy()), n_targets))\n",
        "    preds = np.zeros((len(test_np), n_targets))\n",
        "\n",
        "    worst_targets_in_seed = []\n",
        "    all_targets_ll_in_seed = []\n",
        "\n",
        "    seed_targets = []\n",
        "    seed_oof = []\n",
        "    seed_preds = np.zeros((len(test_np), n_targets, n_fold))\n",
        "    \n",
        "    for fold in range(n_fold):\n",
        "        # print(n, len(tr))\n",
        "        tr_idx = folds_cv != fold\n",
        "        te_idx = folds_cv == fold\n",
        "        xval = train_df.to_numpy()[te_idx]\n",
        "        yval = target_df.to_numpy()[te_idx]\n",
        "        fold_preds = []\n",
        "        \n",
        "        val_set = MoaDataset(xval, yval, top_features)\n",
        "        test_set = MoaDataset(test_np, None, top_features, mode='test')\n",
        "        \n",
        "        dataloaders = {\n",
        "            'val': DataLoader(val_set, batch_size=val_batch_size, shuffle=False),\n",
        "            'test': DataLoader(test_set, batch_size=val_batch_size, shuffle=False)\n",
        "        }\n",
        "        \n",
        "        checkpoint_path = f'Fold:{fold+1}.pt'\n",
        "        model = MoaModel(len(top_features)).to(device)\n",
        "        model.load_state_dict(torch.load(checkpoint_path))\n",
        "        model.eval()\n",
        "        \n",
        "        for phase in ['val', 'test']:\n",
        "            for i, (x, y) in enumerate(dataloaders[phase]):\n",
        "                # print(i)\n",
        "                if phase == 'val':\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                elif phase == 'test':\n",
        "                    x = x.to(device)\n",
        "                \n",
        "                with torch.no_grad():\n",
        "                    batch_preds = model(x)\n",
        "                    \n",
        "                    if phase == 'val':\n",
        "                        seed_targets.append(y)\n",
        "                        seed_oof.append(batch_preds)\n",
        "                        # print(y_pred_lr.shape)\n",
        "                    elif phase == 'test':\n",
        "                        fold_preds.append(batch_preds)\n",
        "                    \n",
        "        fold_preds = torch.cat(fold_preds, dim=0).cpu().numpy()\n",
        "        #print(fold_preds.shape)\n",
        "        seed_preds[:, :,fold] = fold_preds\n",
        "        \n",
        "    seed_targets = torch.cat(seed_targets, dim=0).cpu().numpy()\n",
        "    #print(seed_targets)\n",
        "    seed_oof = torch.cat(seed_oof, dim=0).cpu().numpy()\n",
        "    seed_preds = np.mean(seed_preds, axis=1)\n",
        "    \n",
        "    #print(\"Score for this seed {:5.5f}\".format(mean_log_loss(seed_targets, seed_oof)[0]))\n",
        "    worst_targets_in_seed.append(mean_log_loss(seed_targets, seed_oof)[1])\n",
        "    all_targets_ll_in_seed.append(mean_log_loss(seed_targets, seed_oof)[2])\n",
        "    oof_targets = seed_targets\n",
        "    oof = seed_oof\n",
        "    #oof[:, :] = seed_oof\n",
        "        #preds += seed_preds \n",
        "\n",
        "    #oof = np.mean(oof, axis=1)\n",
        "    #print(\"Overall score is {:5.5f}\".format(mean_log_loss(oof_targets, oof)[0]))\n",
        "    print(oof_targets.shape)\n",
        "    print(oof.shape)\n",
        "    return(mean_log_loss(oof_targets,oof)[0])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s9wbRcXmhOG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "EgrpOuQ4obNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = training(n_epoch=16,tr = 16,learning_rate = 0.001,batch_size = 128)"
      ],
      "metadata": {
        "id": "0f0k5N5hmr6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pbounds = {\n",
        "    'learning_rate': (0.0002, 0.002),\n",
        "    'batch_size': (64,1024),\n",
        "    'tr': (8,20),\n",
        "    'n_epoch': (16,16)\n",
        "    }\n",
        "\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=training,\n",
        "    pbounds=pbounds,\n",
        "    verbose=2,  \n",
        "    random_state=1,\n",
        ")"
      ],
      "metadata": {
        "id": "wnGxruMO2GF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "optimizer.maximize(init_points=16, n_iter=16)\n",
        "end = time.time()\n",
        "print('Bayes optimization takes {:.2f} seconds to tune'.format(end - start))\n",
        "print(optimizer.max)"
      ],
      "metadata": {
        "id": "7XHzncvC21uU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}